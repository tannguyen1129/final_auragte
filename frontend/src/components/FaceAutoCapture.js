"use client";
import { useEffect, useRef, useState } from "react";

export default function FaceAutoCapture({ onCapture, maxCaptures = 5 }) {
  const videoRef = useRef(null);
  const [status, setStatus] = useState("ðŸ”„ Äang khá»Ÿi Ä‘á»™ng camera...");
  const [faceapiLoaded, setFaceapiLoaded] = useState(false);
  const [capturedCount, setCapturedCount] = useState(0);
  const streamRef = useRef(null);
  const intervalRef = useRef(null);

  // Khá»Ÿi Ä‘á»™ng camera & load face-api
  useEffect(() => {
    const init = async () => {
      if (typeof window === "undefined") return;

      const faceapi = await import("@vladmandic/face-api");
      await faceapi.nets.tinyFaceDetector.loadFromUri("/models");

      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      streamRef.current = stream;
      if (videoRef.current) {
        videoRef.current.srcObject = stream;
      }

      setFaceapiLoaded(true);
      setStatus("ðŸŸ¢ ÄÃ£ má»Ÿ camera, Ä‘ang chá» khuÃ´n máº·t...");
    };

    init();

    return () => {
      clearInterval(intervalRef.current);
      if (streamRef.current) {
        streamRef.current.getTracks().forEach((track) => track.stop());
      }
    };
  }, []);

  // Detect khuÃ´n máº·t liÃªn tá»¥c
  useEffect(() => {
    if (!faceapiLoaded || !videoRef.current) return;

    const runDetection = async () => {
      const faceapi = await import("@vladmandic/face-api");

      intervalRef.current = setInterval(async () => {
        if (capturedCount >= maxCaptures) {
          setStatus(`âœ… ÄÃ£ chá»¥p Ä‘á»§ ${maxCaptures} áº£nh.`);
          clearInterval(intervalRef.current);
          return;
        }

        const result = await faceapi.detectSingleFace(
          videoRef.current,
          new faceapi.TinyFaceDetectorOptions()
        );

        if (result) {
          const canvas = document.createElement("canvas");
          canvas.width = videoRef.current.videoWidth;
          canvas.height = videoRef.current.videoHeight;
          const ctx = canvas.getContext("2d");
          ctx.drawImage(videoRef.current, 0, 0);
          const base64 = canvas.toDataURL("image/jpeg").split(",")[1];

          onCapture(base64);
          setCapturedCount((prev) => prev + 1);
          setStatus(`ðŸ“¸ ÄÃ£ chá»¥p ${capturedCount + 1} / ${maxCaptures} áº£nh`);
        }
      }, 1000);
    };

    runDetection();

    return () => clearInterval(intervalRef.current);
  }, [faceapiLoaded, capturedCount, maxCaptures]);

  return (
    <div>
      <video
        ref={videoRef}
        autoPlay
        muted
        className="w-full max-h-48 rounded shadow mb-2"
      />
      <div className="text-sm text-gray-600">{status}</div>
    </div>
  );
}
